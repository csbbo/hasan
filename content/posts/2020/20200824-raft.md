---
title: "Raft算法"
date: 2020-08-24T15:29:37+08:00
categories: ["分布式"]
tags: ["Raft"]
toc: true
---

Raft是一种共识算法，旨在易于理解。具有和Paxos相当容错性和性能，不同的是Raft被分为多个相对独立的子问题：Leader选举（Leader election）、日志同步（Log replication）、安全性（Safety）、日志压缩（Log compaction）、成员变更（Membership change）等，清晰的解决了实际系统所需的所有主要部分。

<!--more-->

### Raft系统中的角色

领导者（Leader）、跟随者（Follower）、候选者（Candidate）：

- **Leader**：接收客户端请求，并向客户端同步请求日志，当日志同步到大多数（过半数）节点后通知**Follower**提交日志
- **Follower**：接收并持久化**Leader**同步过来的日志，收到**Leader**可以提交日志的消息后提交日志
- **Candidate**：Leader选举过程中的临时角色

> Raft要求系统任意时刻最多只有一个Leader，正常工作期间只有Leader和Follower

<img src="/assets/2020/0824/raft1.png" style="width:50%"/>
<span style="margin-left: 37%; color: gray;">Raft算法角色状态转换</span>


> **Follower**响应其他服务器的请求，如果在超时时间内没有收到**Leader**的消息，他会转变成一个**Candidate**并开始一次**Leader**选举，而收到大多数（过半数）服务器的投票后Candidate会成为新的**Leader**并一直保持**Leader**的状态直到宕机


### Leader选举

<img src="/assets/2020/0824/raft2.png" style="width:50%"/>

**Raft**算法将时间分为一个个的任期（term），每个term的开始都是**Leader**的选举，选举成功**Leader**在整个term内管理整个集群，如果选举失败该term会因为没有**Leader**而结束

**Raft**使用心跳（heartbeat)触发**Leader**选举，服务器启动时所有节点初始化为**Follower**，**Leader**选举成功后向所有**Follower**周期性发送**heartbeat**，如果**Follower**在选举超时时间内没有收到**heartbeat**就会等待一段随机时间后发起一次**Leader**选举，**Follower**将当前的**term**加1然后转换为**Candidate**，接下来它首先给自己投票并且给集群其他服务器发送**RequestVote RPC**
> 每个节点只有1票

**Candidate**向其他节点请求投票后有以下三种情况：

- 获得多数的选票成为**Leader**
- 收到**Leader**的**heartbeat**，其他节点已经抢先成为**Leader**
- 没有节点获得多数选票，选举失败，等待选举时间超时后发起下一次的选举

<img src="/assets/2020/0824/raft3.png" style="width:50%"/>

选举出**Leader**后，**Leader**通过定期向所有**Follower**发送**heartbeat**信息维持其统治。若**Follower**一段时间未收到**Leader**的**heartbeat**则认为**Leader**可能已经挂了，再次发起**Leader**选举过程。
> **Raft**会保证选举出的**Leader**上一定具有最新的已提交的日志

### 日志同步

**Leader**选出后，就开始接收客户端的请求。**Leader**把请求作为日志条目（Log entries）加入到它的日志中，然后并行的向其他服务器发起AppendEntries RPC复制日志条目。当这条日志被复制到大多数服务器上，Leader将这条日志应用到它的状态机并向客户端返回执行结果。

<img src="/assets/2020/0824/raft4.png" style="width:60%"/>
<span style="margin-left: 40%; color: gray;">Raft日志同步过程</span>

> 某些Follower可能没有成功的复制日志，Leader会无限的重试 AppendEntries RPC直到所有的Followers最终存储了所有的日志条目

日志由log index的日志条目组成，每个日志条目包含任期号和用于状态机执行的命令，如果日志条目被复制到大多数节点则被认为可以提交

Raft日志同步保证：

- 如果不同日志中的两个条目有着相同的索引和任期号，则它们所存储的命令是相同的
- 如果不同日志中的两个条目有着相同的索引和任期号，则它们之前的所有条目都是完全一样的

第一条特性源于Leader在一个term内在给定的一个log index最多创建一条日志条目，同时该条目在日志中的位置也从来不会改变

第二条特性源于 AppendEntries 的一个简单的一致性检查。当发送一个 AppendEntries RPC 时，Leader会把新日志条目紧接着之前的条目的log index和term都包含在里面。如果Follower没有在它的日志中找到log index和term都相同的日志，它就会拒绝新的日志条目

<img src="/assets/2020/0824/raft5.png" style="width:60%"/>
<span style="margin-left: 30%; color: gray;">Leader和Follower日志不一致</span>

- 上图**Follower**可能和新的Leader日志不同的情况。一个**Follower**可能会丢失掉**Leader**上的一些条目，也有可能包含一些**Leader**没有的条目，也有可能两者都会发生。丢失的或者多出来的条目可能会持续多个任期。

- **Leader**通过强制**Follower**复制它的日志来处理日志的不一致，**Follower**上的不一致的日志会被**Leader**的日志覆盖。

- **Leader**为了使**Follower**的日志同自己的一致，**Leader**需要找到**Follower**同它的日志一致的地方，然后覆盖Followers在该位置之后的条目。

- **Leader**会从后往前试，每次**AppendEntries**失败后尝试前一个日志条目，直到成功找到每个**Follower**的日志一致位点，然后向后逐条覆盖**Follower**在该位置之后的条目。

### 安全性

Raft增加了如下两条限制以保证安全性：

- 拥有最新的已提交的log entry的Follower才有资格成为Leader

这个保证是在RequestVote RPC中做的，Candidate在发送RequestVote RPC时，要带上自己的最后一条日志的term和log index，其他节点收到消息时，如果发现自己的日志比请求中携带的更新，则拒绝投票。日志比较的原则是，如果本地的最后一条log entry的term更大，则term大的更新，如果term一样大，则log index更大的更新

- Leader只能推进commit index来提交当前term的已经复制到大多数服务器上的日志，旧term日志的提交要等到提交当前term的日志来间接提交（log index 小于 commit index的日志被间接提交）

之所以要这样，是因为可能会出现已提交的日志又被覆盖的情况

<img src="/assets/2020/0824/raft6.png" style="width:60%"/>
<span style="margin-left: 40%; color: gray;">已提交的日志被覆盖</span>

- 在阶段a，term为2，S1是Leader，且S1写入日志（term, index）为(2, 2)，并且日志被同步写入了S2；

- 在阶段b，S1离线，触发一次新的选主，此时S5被选为新的Leader，此时系统term为3，且写入了日志（term, index）为（3， 2）;

- S5尚未将日志推送到Followers就离线了，进而触发了一次新的选主，而之前离线的S1经过重新上线后被选中变成Leader，此时系统term为4，此时S1会将自己的日志同步到Followers，按照上图就是将日志（2， 2）同步到了S3，而此时由于该日志已经被同步到了多数节点（S1, S2, S3），因此，此时日志（2，2）可以被提交了。；

- 在阶段d，S1又下线了，触发一次选主，而S5有可能被选为新的Leader（这是因为S5可以满足作为主的一切条件：1. term = 5 > 4，2. 最新的日志为（3，2），比大多数节点（如S2/S3/S4的日志都新），然后S5会将自己的日志更新到Followers，于是S2、S3中已经被提交的日志（2，2）被截断了。

- 增加上述限制后，即使日志（2，2）已经被大多数节点（S1、S2、S3）确认了，但是它不能被提交，因为它是来自之前term（2）的日志，直到S1在当前term（4）产生的日志（4， 4）被大多数Followers确认，S1方可提交日志（4，4）这条日志，当然，根据Raft定义，（4，4）之前的所有日志也会被提交。此时即使S1再下线，重新选主时S5不可能成为Leader，因为它没有包含大多数节点已经拥有的日志（4，4）

### 日志压缩

在实际的系统中，不能让日志无限增长，否则系统重启时需要花很长的时间进行回放，从而影响可用性。Raft采用对整个系统进行snapshot来解决，snapshot之前的日志都可以丢弃。

每个副本独立的对自己的系统状态进行snapshot，并且只能对已经提交的日志记录进行snapshot。

Snapshot中包含以下内容：

- 日志元数据。最后一条已提交的 log entry的 log index和term。这两个值在snapshot之后的第一条log entry的AppendEntries RPC的完整性检查的时候会被用上。
- 系统当前状态。
当Leader要发给某个日志落后太多的Follower的log entry被丢弃，Leader会将snapshot发给Follower。或者当新加进一台机器时，也会发送snapshot给它。发送snapshot使用InstalledSnapshot RPC（RPC细节参见八、Raft算法总结）。

做snapshot既不要做的太频繁，否则消耗磁盘带宽， 也不要做的太不频繁，否则一旦节点重启需要回放大量日志，影响可用性。推荐当日志达到某个固定的大小做一次snapshot。

做一次snapshot可能耗时过长，会影响正常日志同步。可以通过使用copy-on-write技术避免snapshot过程影响正常日志同步。

### 成员变更

成员变更是在集群运行过程中副本发生变化，如增加/减少副本数、节点替换等。

成员变更也是一个分布式一致性问题，既所有服务器对新成员达成一致。但是成员变更又有其特殊性，因为在成员变更的一致性达成的过程中，参与投票的进程会发生变化。

如果将成员变更当成一般的一致性问题，直接向Leader发送成员变更请求，Leader复制成员变更日志，达成多数派之后提交，各服务器提交成员变更日志后从旧成员配置（Cold）切换到新成员配置（Cnew）。

因为各个服务器提交成员变更日志的时刻可能不同，造成各个服务器从旧成员配置（Cold）切换到新成员配置（Cnew）的时刻不同。

成员变更不能影响服务的可用性，但是成员变更过程的某一时刻，可能出现在Cold和Cnew中同时存在两个不相交的多数派，进而可能选出两个Leader，形成不同的决议，破坏安全性。

<img src="/assets/2020/0824/raft7.png" style="width:60%"/>
<span style="margin-left: 21%; color: gray;">成员变更的某一时刻Cold和Cnew中同时存在两个不相交的多数派</span>

由于成员变更的这一特殊性，成员变更不能当成一般的一致性问题去解决。

为了解决这一问题，Raft提出了两阶段的成员变更方法。集群先从旧成员配置Cold切换到一个过渡成员配置，称为共同一致（joint consensus），共同一致是旧成员配置Cold和新成员配置Cnew的组合Cold U Cnew，一旦共同一致Cold U Cnew被提交，系统再切换到新成员配置Cnew

### 动态演示

<iframe src="https://raft.github.io/raftscope/index.html" title="raft visualization" aria-hidden="true" style="border: 0; width: 800px; height:580px; margin-bottom: 20px">
</iframe>

[参考]

[Paxos、Raft分布式一致性最佳实践](https://zhuanlan.zhihu.com/p/32052223)  
[The Raft Consensus Algorithm](https://raft.github.io/)

<!-- https://zhuanlan.zhihu.com/p/27207160
https://zhuanlan.zhihu.com/p/91288179 -->
